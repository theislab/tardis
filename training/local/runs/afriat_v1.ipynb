{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a174ddf-1bac-4261-a2df-155cc7e8fc31",
   "metadata": {},
   "source": [
    "# Afriat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd13e0f4-b441-4653-bf08-f2812d58f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d362b42-a27d-486f-9c83-f52a59bfc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import copy\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "sys.path.append(\"/Users/kemalinecik/git_nosync/tardis\")\n",
    "import tardis\n",
    "tardis.config = tardis.config_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e834108-5b88-402d-94c9-c8002c7a927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA used: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA used: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a8fa63-54f9-4cc9-aa2c-e2f1ce8ec9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "_rcparams_path = \"/Users/kemalinecik/git_nosync/tardis/training/local/figures/rcparams.pickle\"\n",
    "with open(_rcparams_path, 'rb') as file:\n",
    "    _rcparams = pickle.load(file)\n",
    "plt.rcParams.update(_rcparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea9ee642-93ef-4cbc-b83c-5e9f16122b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 19053 × 5000\n",
       "    obs: 'mouse', 'experiment', 'time_int', 'time_cat', 'zone', 'status_control'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_file_path = os.path.join(tardis.config.io_directories[\"processed\"], \"biolord_afriat.h5ad\")\n",
    "assert os.path.isfile(adata_file_path), f\"File not already exist: `{adata_file_path}`\"\n",
    "adata = ad.read_h5ad(adata_file_path)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd461c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_level_metrics = [\n",
    "    dict(\n",
    "        metric_identifier = \"metric_mi|status_control\",\n",
    "        training_set = [\"train\"],\n",
    "        every_n_epoch = 5,\n",
    "        subsample = 1.0,\n",
    "        progress_bar = True,\n",
    "        metric_kwargs = dict(\n",
    "            variation = \"normalized\",\n",
    "            discretization_bins = 256,\n",
    "            latent_subset=None,\n",
    "            reduce=np.mean\n",
    "        )\n",
    "    ),\n",
    "    dict(\n",
    "        metric_identifier = \"metric_mi|zone\",\n",
    "        training_set = [\"train\"],\n",
    "        every_n_epoch = 5,\n",
    "        subsample = 1.0,\n",
    "        progress_bar = True,\n",
    "        metric_kwargs = dict(\n",
    "            variation = \"normalized\",\n",
    "            discretization_bins = 128,\n",
    "            latent_subset=None,\n",
    "            reduce=np.mean\n",
    "        )\n",
    "    ),\n",
    "    dict(\n",
    "        metric_identifier = \"metric_mi|time_cat\",\n",
    "        training_set = [\"train\"],\n",
    "        every_n_epoch = 5,\n",
    "        subsample = 1.0,\n",
    "        progress_bar = True,\n",
    "        metric_kwargs = dict(\n",
    "            variation = \"normalized\",\n",
    "            discretization_bins = 128,\n",
    "            latent_subset=None,\n",
    "            reduce=np.mean\n",
    "        )\n",
    "    ),\n",
    "    dict(\n",
    "        metric_identifier = \"metric_mi|mouse\",\n",
    "        training_set = [\"train\"],\n",
    "        every_n_epoch = 5,\n",
    "        subsample = 1.0,\n",
    "        progress_bar = True,\n",
    "        metric_kwargs = dict(\n",
    "            variation = \"normalized\",\n",
    "            discretization_bins = 128,\n",
    "            latent_subset=None,\n",
    "            reduce=np.mean\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43333cc-9e19-469f-8452-11ff5ee08db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_epoch_range = [6, 48]\n",
    "dtc_w1 = 100\n",
    "dtc_w2 = 10\n",
    "\n",
    "counteractive_minibatch_settings = dict(\n",
    "    method = \"categorical_random\",\n",
    "    method_kwargs = dict(\n",
    "        within_labels = False,\n",
    "        within_batch = False,\n",
    "        within_categorical_covs = [False, False],\n",
    "        seed = \"forward\",\n",
    "    )\n",
    ")\n",
    "\n",
    "disentenglement_targets_configurations=[\n",
    "    dict(\n",
    "        obs_key = \"status_control\",\n",
    "        n_reserved_latent = 8,\n",
    "        counteractive_minibatch_settings = counteractive_minibatch_settings,\n",
    "        auxillary_losses = [\n",
    "            dict(\n",
    "                apply = True, \n",
    "                target_type=\"categorical\",\n",
    "                progress_bar = True,\n",
    "                weight = dtc_w1,\n",
    "                method = \"mse_z\", \n",
    "                latent_group = \"reserved\",\n",
    "                counteractive_example = \"negative\",\n",
    "                transformation = \"inverse\", \n",
    "                warmup_epoch_range=warmup_epoch_range,\n",
    "                method_kwargs = {}\n",
    "            ),\n",
    "            dict(\n",
    "                apply = True, \n",
    "                target_type=\"categorical\",\n",
    "                progress_bar = True,\n",
    "                weight = dtc_w2, \n",
    "                method = \"mse_z\", \n",
    "                latent_group = \"reserved\",\n",
    "                counteractive_example = \"positive\",\n",
    "                transformation = \"none\",\n",
    "                warmup_epoch_range=warmup_epoch_range,\n",
    "                method_kwargs = {}\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1962de-51e8-453c-907a-b338c1f82f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kemalinecik/tools/apps/mamba/envs/tardis_env/lib/python3.10/site-packages/scvi/data/fields/_base_field.py:64: UserWarning: adata.X does not contain unnormalized count data. Are you sure this is what you want?\n",
      "  self.validate_field(adata)\n",
      "/var/folders/y7/7c17s0l57szdjc1cdc9dmnpm0000gn/T/ipykernel_99265/1210881523.py:50: UserWarning: `(None, ['zone', 'time_cat'])` is defined as `batch_key` or `categorical_covariate`, it will be given to both encoder and decoder. Make sure this does not contain information of any of your disentenglement targets. For example, if `donor_id` is chosen as a batch key, do not disentengle donor level information such as `sex` or `age`. The decoder should not use the disentengled latent spaces, simply ignores, if it is already given a batch_key. \n",
      "  tardis.MyModel.setup_anndata(adata, **dataset_params)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/kemalinecik/tools/apps/mamba/envs/tardis_env/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:201: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "/Users/kemalinecik/tools/apps/mamba/envs/tardis_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B logger initialized with the following parameters: \n",
      "Entity: inecik-academic\n",
      "Project: tardis_figures\n",
      "ID: h2rmu3x0\n",
      "Name: olive-snowflake-20\n",
      "Tags: tardis, conference, figures, final\n",
      "Notes: Final runs for Tardis before conference.\n",
      "URL: https://wandb.ai/inecik-academic/tardis_figures/runs/h2rmu3x0\n",
      "Directory: /Users/kemalinecik/wandb/run-20240430_164742-h2rmu3x0/files\n",
      "\n",
      "Epoch 1/600:   0%|          | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kemalinecik/git_nosync/tardis/tardis/_counteractivegenerator.py:232: UserWarning: Possible group definition indices are calculated for `status_control` for `training` set. Number of elements in each group: 2549,3243,9451\n",
      "  possible_indices = CachedPossibleGroupDefinitionIndices.get(\n",
      "/Users/kemalinecik/git_nosync/tardis/tardis/_counteractivegenerator.py:232: UserWarning: Possible group definition indices are calculated for `status_control` for `validation` set. Number of elements in each group: 656,866,2288\n",
      "  possible_indices = CachedPossibleGroupDefinitionIndices.get(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/600:   0%|          | 3/600 [00:24<1:19:32,  7.99s/it, v_num=x0_1, total_loss_train=3.22e+3, kl_local_train=58.6, tardis_status_control_0_train=33.5, tardis_status_control_1_train=24.3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kemalinecik/tools/apps/mamba/envs/tardis_env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:53: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B logger finalized successfully: \n",
      "Exit Code: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs_kl_warmup = 600\n",
    "\n",
    "model_params = dict(\n",
    "    n_hidden=512,\n",
    "    n_layers=3, \n",
    "    n_latent=(24 + 8 * len(disentenglement_targets_configurations)),\n",
    "    gene_likelihood = \"nb\",\n",
    "    use_batch_norm = \"none\",\n",
    "    use_layer_norm = \"both\",\n",
    "    dropout_rate = 0.25,\n",
    "    deeply_inject_disentengled_latents = True,\n",
    "    include_auxillary_loss = True,\n",
    "    beta_kl_weight = 0.5,\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    max_epochs=600,\n",
    "    train_size=0.8,\n",
    "    batch_size=128,\n",
    "    check_val_every_n_epoch=10,\n",
    "    limit_train_batches=1.0, \n",
    "    limit_val_batches=1.0,\n",
    "    learning_rate_monitor=True,\n",
    "    # early stopping:\n",
    "    early_stopping=False,\n",
    "    early_stopping_patience=150,\n",
    "    early_stopping_monitor=\"elbo_train\",\n",
    "    plan_kwargs = dict(\n",
    "        n_epochs_kl_warmup=n_epochs_kl_warmup,\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        optimizer=\"AdamW\",\n",
    "        # lr-scheduler:\n",
    "        reduce_lr_on_plateau=True,\n",
    "        lr_patience=100,\n",
    "        lr_scheduler_metric=\"elbo_train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset_params = dict(\n",
    "    layer=None, \n",
    "    labels_key=None,\n",
    "    batch_key=None,\n",
    "    categorical_covariate_keys=['zone', 'time_cat'],\n",
    "    disentenglement_targets_configurations=disentenglement_targets_configurations,\n",
    "    model_level_metrics=model_level_metrics,\n",
    "    model_level_metrics_helper_covariates=['zone', 'status_control', 'time_cat', 'mouse']\n",
    ")\n",
    "\n",
    "tardis.MyModel.setup_anndata(adata, **dataset_params)\n",
    "dataset_params[\"adata_path\"] = adata_file_path\n",
    "dataset_params[\"adata\"] = os.path.split(adata_file_path)[1]\n",
    "\n",
    "tardis.MyModel.setup_wandb(\n",
    "    wandb_configurations=tardis.config.wandb,\n",
    "    hyperparams=dict(\n",
    "        model_params=model_params,\n",
    "        train_params=train_params,\n",
    "        dataset_params=dataset_params,\n",
    "    )\n",
    ")\n",
    "\n",
    "vae = tardis.MyModel(\n",
    "    adata,\n",
    "    **model_params\n",
    ")\n",
    "vae.train(**train_params)\n",
    "\n",
    "dir_path = os.path.join(\n",
    "    tardis.config.io_directories[\"models\"],\n",
    "    \"afriat_v1\"\n",
    ")\n",
    "\n",
    "vae.save(\n",
    "    dir_path,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4f365-b7a0-4c4d-92b9-993c53327048",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.get_reconstruction_r2_training(top_n=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = ad.AnnData(X=vae.get_latent_representation(), obs=adata.obs.copy())\n",
    "sc.pp.neighbors(latent, n_neighbors = 30)\n",
    "sc.tl.umap(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04982e03-16d4-4a56-894b-5b1ff00f5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.plot_training_history(\n",
    "    ignore_first=0,\n",
    "    n_col=4,\n",
    "    metrics_name=[\"metric_mi|status_control\", \"metric_mi|zone\", \"metric_mi|time_cat\", \"metric_mi|mouse\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8263757-7de1-41b9-b555-76f5e38bfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.plot_training_history(\n",
    "    ignore_first=0,\n",
    "    n_col=4,\n",
    "    metrics_name=[\"reconstruction_loss\", \"kl_local\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6519c-3683-4091-982f-329434f56c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sc.pl.umap(\n",
    "        latent, \n",
    "        color=['mouse', 'experiment', 'time_int', 'time_cat', 'zone', 'status_control'], \n",
    "        ncols=2,\n",
    "        frameon=False,\n",
    "        legend_fontsize=\"xx-small\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9986c-60db-49f3-b386-689f8310579b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
