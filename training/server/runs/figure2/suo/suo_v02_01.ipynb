{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a174ddf-1bac-4261-a2df-155cc7e8fc31",
   "metadata": {},
   "source": [
    "# Suo v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd13e0f4-b441-4653-bf08-f2812d58f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d362b42-a27d-486f-9c83-f52a59bfc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import copy\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "sys.path.append(\"/home/icb/kemal.inecik/work/codes/tardis\")\n",
    "import tardis\n",
    "tardis.config = tardis.config_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309fed90-544a-4975-ad9e-57364532be75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100-PCIE-40GB MIG 3g.20gb\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No CUDA GPUs are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a8fa63-54f9-4cc9-aa2c-e2f1ce8ec9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "_rcparams_path = \"/home/icb/kemal.inecik/work/codes/tardis/training/local/figures/rcparams.pickle\"\n",
    "with open(_rcparams_path, 'rb') as file:\n",
    "    _rcparams = pickle.load(file)\n",
    "plt.rcParams.update(_rcparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea9ee642-93ef-4cbc-b83c-5e9f16122b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 841922 × 8192\n",
       "    obs: 'sample_ID', 'organ', 'age', 'cell_type', 'sex', 'sex_inferred', 'concatenated_integration_covariates', 'integration_donor', 'integration_biological_unit', 'integration_sample_status', 'integration_library_platform_coarse', 'n_genes'\n",
       "    uns: 'rank_genes_groups'\n",
       "    obsm: 'Unintegrated', 'X_pca', 'harmony'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_file_path = os.path.join(tardis.config.io_directories[\"processed\"], \"dataset_complete_Suo.h5ad\")\n",
    "assert os.path.isfile(adata_file_path), f\"File not already exist: `{adata_file_path}`\"\n",
    "adata = ad.read_h5ad(adata_file_path)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd461c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_level_metrics = [\n",
    "    dict(\n",
    "        metric_identifier = \"metric_mi|integration_library_platform_coarse\",\n",
    "        training_set = [\"train\", \"validation\"],\n",
    "        every_n_epoch = 5,\n",
    "        subsample = 0.25,\n",
    "        progress_bar = True,\n",
    "        metric_kwargs = dict(\n",
    "            variation = \"normalized\",\n",
    "            discretization_bins = 512,\n",
    "            latent_subset=None,\n",
    "            reduce=np.mean\n",
    "        )\n",
    "    ),\n",
    "    dict(\n",
    "        metric_identifier = \"metric_mi|integration_donor\",\n",
    "        training_set = [\"train\", \"validation\"],\n",
    "        every_n_epoch = 5,\n",
    "        subsample = 0.25,\n",
    "        progress_bar = True,\n",
    "        metric_kwargs = dict(\n",
    "            variation = \"normalized\",\n",
    "            discretization_bins = 512,\n",
    "            latent_subset=None,\n",
    "            reduce=np.mean\n",
    "        )\n",
    "    ),\n",
    "    dict(\n",
    "        metric_identifier = \"metric_mi|organ\",\n",
    "        training_set = [\"train\", \"validation\"],\n",
    "        every_n_epoch = 5,\n",
    "        subsample = 0.25,\n",
    "        progress_bar = True,\n",
    "        metric_kwargs = dict(\n",
    "            variation = \"normalized\",\n",
    "            discretization_bins = 512,\n",
    "            latent_subset=None,\n",
    "            reduce=np.mean\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a11c7a5-b45f-4a9f-a273-c22f8826eb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_epoch_range = [6, 48]\n",
    "dtc_w1 = 100\n",
    "dtc_w2 = 10\n",
    "\n",
    "counteractive_minibatch_settings = dict(\n",
    "    method = \"categorical_random\",\n",
    "    method_kwargs = dict(\n",
    "        within_labels = False,\n",
    "        within_batch = False,\n",
    "        within_categorical_covs = [False],\n",
    "        seed = \"forward\",\n",
    "    )\n",
    ")\n",
    "\n",
    "disentenglement_targets_configurations=[\n",
    "    dict(\n",
    "        obs_key = \"integration_library_platform_coarse\",\n",
    "        n_reserved_latent = 8,\n",
    "        counteractive_minibatch_settings = counteractive_minibatch_settings,\n",
    "        auxillary_losses = [\n",
    "            dict(\n",
    "                apply = True, \n",
    "                target_type=\"categorical\",\n",
    "                progress_bar = True,\n",
    "                weight = dtc_w1,\n",
    "                method = \"mse_z\", \n",
    "                latent_group = \"reserved\",\n",
    "                counteractive_example = \"negative\",\n",
    "                transformation = \"inverse\", \n",
    "                warmup_epoch_range=warmup_epoch_range,\n",
    "                method_kwargs = {}\n",
    "            ),\n",
    "            dict(\n",
    "                apply = True, \n",
    "                target_type=\"categorical\",\n",
    "                progress_bar = True,\n",
    "                weight = dtc_w2, \n",
    "                method = \"mse_z\", \n",
    "                latent_group = \"reserved\",\n",
    "                counteractive_example = \"positive\",\n",
    "                transformation = \"none\",\n",
    "                warmup_epoch_range=warmup_epoch_range,\n",
    "                method_kwargs = {}\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1962de-51e8-453c-907a-b338c1f82f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found cuDNN version 8700, but JAX was built against version 8907, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/tmp/ipykernel_411730/2763245126.py:50: UserWarning: `(None, ['integration_donor'])` is defined as `batch_key` or `categorical_covariate`, it will be given to both encoder and decoder. Make sure this does not contain information of any of your disentenglement targets. For example, if `donor_id` is chosen as a batch key, do not disentengle donor level information such as `sex` or `age`. The decoder should not use the disentengled latent spaces, simply ignores, if it is already given a batch_key. \n",
      "  tardis.MyModel.setup_anndata(adata, **dataset_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B logger initialized with the following parameters: \n",
      "Entity: inecik-academic\n",
      "Project: tardis_figures\n",
      "ID: crsoxzuv\n",
      "Name: unique-gorge-1\n",
      "Tags: tardis, conference, figures, final\n",
      "Notes: Final runs for Tardis before conference.\n",
      "URL: https://wandb.ai/inecik-academic/tardis_figures/runs/crsoxzuv\n",
      "Directory: /lustre/groups/ml01/workspace/kemal.inecik/wandb/run-20240430_150930-crsoxzuv/files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB MIG 3g.20gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-d7a14d27-a4ce-50f4-ac0d-9574c956ed5a,MIG-c1e0b253-b0df-5a6c-88e9-2330111f3783]\n",
      "/home/icb/kemal.inecik/tools/apps/mamba/envs/tardis_env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600:   0%|                                                                                                                                          | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/kemal.inecik/work/codes/tardis/tardis/_counteractivegenerator.py:232: UserWarning: Possible group definition indices are calculated for `integration_library_platform_coarse` for `training` set. Number of elements in each group: 317480,356058\n",
      "  possible_indices = CachedPossibleGroupDefinitionIndices.get(\n",
      "/home/icb/kemal.inecik/work/codes/tardis/tardis/_counteractivegenerator.py:232: UserWarning: Possible group definition indices are calculated for `integration_library_platform_coarse` for `validation` set. Number of elements in each group: 79561,88823\n",
      "  possible_indices = CachedPossibleGroupDefinitionIndices.get(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/600:   2%| | 11/600 [06:39<4:23:06, 26.80s/it, v_num=uv_1, total_loss_train=422, kl_local_train=77.1, tardis_integration_library_platform_coarse_0_train=21.9, tardis_integ"
     ]
    }
   ],
   "source": [
    "n_epochs_kl_warmup = 600\n",
    "\n",
    "model_params = dict(\n",
    "    n_hidden=512,\n",
    "    n_layers=3, \n",
    "    n_latent=24, \n",
    "    gene_likelihood = \"nb\",\n",
    "    use_batch_norm = \"none\",\n",
    "    use_layer_norm = \"both\",\n",
    "    dropout_rate = 0.25,\n",
    "    deeply_inject_disentengled_latents = False,\n",
    "    include_auxillary_loss = True,\n",
    "    beta_kl_weight = 0.5,\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    max_epochs=600,\n",
    "    train_size=0.8,\n",
    "    batch_size=128,\n",
    "    check_val_every_n_epoch=10,\n",
    "    limit_train_batches=0.2, \n",
    "    limit_val_batches=0.2,\n",
    "    learning_rate_monitor=True,\n",
    "    # early stopping:\n",
    "    early_stopping=False,\n",
    "    early_stopping_patience=150,\n",
    "    early_stopping_monitor=\"elbo_train\",\n",
    "    plan_kwargs = dict(\n",
    "        n_epochs_kl_warmup=n_epochs_kl_warmup,\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        optimizer=\"AdamW\",\n",
    "        # lr-scheduler:\n",
    "        reduce_lr_on_plateau=True,\n",
    "        lr_patience=100,\n",
    "        lr_scheduler_metric=\"elbo_train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset_params = dict(\n",
    "    layer=None, \n",
    "    labels_key=\"cell_type\",\n",
    "    batch_key=None,\n",
    "    categorical_covariate_keys=[\"integration_donor\"],\n",
    "    disentenglement_targets_configurations=disentenglement_targets_configurations,\n",
    "    model_level_metrics=model_level_metrics,\n",
    "    model_level_metrics_helper_covariates=['integration_library_platform_coarse', 'integration_donor', 'organ']\n",
    ")\n",
    "\n",
    "tardis.MyModel.setup_anndata(adata, **dataset_params)\n",
    "dataset_params[\"adata_path\"] = adata_file_path\n",
    "dataset_params[\"adata\"] = os.path.split(adata_file_path)[1]\n",
    "\n",
    "tardis.MyModel.setup_wandb(\n",
    "    wandb_configurations=tardis.config.wandb,\n",
    "    hyperparams=dict(\n",
    "        model_params=model_params,\n",
    "        train_params=train_params,\n",
    "        dataset_params=dataset_params,\n",
    "    )\n",
    ")\n",
    "\n",
    "vae = tardis.MyModel(\n",
    "    adata,\n",
    "    **model_params\n",
    ")\n",
    "vae.train(**train_params)\n",
    "\n",
    "dir_path = os.path.join(\n",
    "    tardis.config.io_directories[\"models\"],\n",
    "    \"suo_v02_01\"\n",
    ")\n",
    "\n",
    "vae.save(\n",
    "    dir_path,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c22472-557a-4aa4-85a7-0dbfd6ba1e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
