{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# # this may cause DisentenglementTargetManager to reimported, losing all the data e.g. configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()).resolve().parents[0]))\n",
    "\n",
    "import tardis\n",
    "\n",
    "local_run = False\n",
    "if local_run:\n",
    "    tardis.config = tardis.config_local\n",
    "else:\n",
    "    tardis.config = tardis.config_server\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_file_path = os.path.join(tardis.config.io_directories[\"processed\"], \"dataset_subset_sample_status_1.h5ad\")\n",
    "assert os.path.isfile(adata_file_path), f\"File not already exist: `{adata_file_path}`\"\n",
    "adata = ad.read_h5ad(adata_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found cuDNN version 8700, but JAX was built against version 8800, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|                                                                                                                    | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/kemal.inecik/work/codes/tardis/tardis/_counteractiveminibatchgenerator.py:56: UserWarning: Possible group definition indices are calculating for `integration_sample_status`.\n",
      "  possible_indices = CachedPossibleGroupDefinitionIndices.get(\n",
      "/home/icb/kemal.inecik/work/codes/tardis/tardis/_counteractiveminibatchgenerator.py:56: UserWarning: Number of elements in each group for `integration_sample_status`: 216,241,1325,2284,70,399,475,4014\n",
      "  possible_indices = CachedPossibleGroupDefinitionIndices.get(\n",
      "/home/icb/kemal.inecik/work/codes/tardis/tardis/_counteractiveminibatchgenerator.py:56: UserWarning: Possible group definition indices are calculating for `sample_ID`.\n",
      "  possible_indices = CachedPossibleGroupDefinitionIndices.get(\n",
      "/home/icb/kemal.inecik/work/codes/tardis/tardis/_counteractiveminibatchgenerator.py:56: UserWarning: Number of elements in each group for `sample_ID`: 70,399,475,4014,216,241,1325,2284\n",
      "  possible_indices = CachedPossibleGroupDefinitionIndices.get(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|█████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.15s/it, v_num=x7_1, train_loss_step=129, train_loss_epoch=118]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|█████████████████████████████████████████████████| 100/100 [01:58<00:00,  1.19s/it, v_num=x7_1, train_loss_step=129, train_loss_epoch=118]\n",
      "W&B logger finalized with the following parameters: \n",
      "Exit Code: 0\n",
      "Entity: inecik-academic\n",
      "Project: tardis_experimental_runs\n",
      "ID: bmbehex7\n",
      "Name: young-gorge-29\n",
      "Tags: tardis, experimental, development\n",
      "Notes: Development runs for tardis.\n",
      "URL: https://wandb.ai/inecik-academic/tardis_experimental_runs/runs/bmbehex7\n",
      "Directory: /home/icb/kemal.inecik/work/codes/tardis/training/server/wandb/run-20240408_140438-bmbehex7/files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "disentenglement_targets_configurations=[\n",
    "    dict(\n",
    "        obs_key = \"integration_sample_status\",\n",
    "        n_reserved_latent = 3,\n",
    "        counteractive_minibatch_settings = dict(\n",
    "            method = \"random\",\n",
    "            method_kwargs = dict(\n",
    "                within_labels = True,\n",
    "                within_batch = True,\n",
    "                within_categorical_covs = [True, False],\n",
    "                within_other_groups = True,\n",
    "                seed = \"gglobal\",\n",
    "            )\n",
    "        ),\n",
    "        auxillary_losses = dict(\n",
    "            loss_complete_latent = dict(\n",
    "                apply = True, \n",
    "                method = \"mse\", \n",
    "                weight = 1.0, \n",
    "                negative_sign = True, \n",
    "                method_kwargs = {}\n",
    "            ),\n",
    "            loss_subset_latent = dict(\n",
    "                apply = False, \n",
    "                method = \"cross_entropy\", \n",
    "                weight = 2.0, \n",
    "                negative_sign = True, \n",
    "                method_kwargs = {}\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    dict(\n",
    "        obs_key = \"sample_ID\",\n",
    "        n_reserved_latent = 5,\n",
    "        counteractive_minibatch_settings = dict(\n",
    "            method = \"random\",\n",
    "            method_kwargs = dict(\n",
    "                within_labels = True,\n",
    "                within_batch = True,\n",
    "                within_categorical_covs = [True, False],\n",
    "                within_other_groups = True,\n",
    "                seed = \"gglobal\",\n",
    "            )\n",
    "        ),\n",
    "        auxillary_losses = dict(\n",
    "            loss_complete_latent = dict(\n",
    "                apply = True, \n",
    "                method = \"mse\", \n",
    "                weight = 1.0, \n",
    "                negative_sign = True, \n",
    "                method_kwargs = {}\n",
    "            ),\n",
    "            loss_subset_latent = dict(\n",
    "                apply = False, \n",
    "                method = \"cross_entropy\", \n",
    "                weight = 2.0, \n",
    "                negative_sign = True, \n",
    "                method_kwargs = {}\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "]\n",
    "\n",
    "model_params = dict(\n",
    "    n_hidden=256,\n",
    "    n_layers=3, \n",
    "    n_latent=20, \n",
    "    gene_likelihood=\"nb\",\n",
    "    dropout_rate = 0.1\n",
    ")\n",
    "train_params = dict(\n",
    "    max_epochs=100,\n",
    "    train_size=0.8\n",
    ")\n",
    "dataset_params = dict(\n",
    "    layer=None, \n",
    "    labels_key=\"cell_type\",\n",
    "    batch_key=\"concatenated_integration_covariates\",\n",
    "    categorical_covariate_keys=[\"sex\", \"age\"],\n",
    "    disentenglement_targets_configurations=disentenglement_targets_configurations\n",
    ")\n",
    "\n",
    "tardis.MyModel.setup_anndata(adata, **dataset_params)\n",
    "\n",
    "tardis.MyModel.setup_wandb(\n",
    "    wandb_configurations=tardis.config.wandb,\n",
    "    hyperparams=dict(\n",
    "        model_params=model_params,\n",
    "        train_params=train_params,\n",
    "        dataset_params=dataset_params,\n",
    "    )\n",
    ")\n",
    "\n",
    "vae = tardis.MyModel(adata, **model_params)\n",
    "vae.train(**train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note: Debugging takes significant amount of time__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tardis._DEBUG import DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'batch', 'disentenglement_target', 'extra_categorical_covs', 'labels', 'disentenglement_target_tensors'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG.tensors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['integration_sample_status', 'sample_ID'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG.tensors[\"disentenglement_target_tensors\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'batch', 'disentenglement_target', 'extra_categorical_covs', 'labels'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG.tensors[\"disentenglement_target_tensors\"][\"sample_ID\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm[\"X_scVI\"] = vae.get_latent_representation()\n",
    "sc.pp.neighbors(adata, n_neighbors = 30, use_rep=\"X_scVI\")\n",
    "sc.tl.umap(adata, min_dist=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sc.pl.umap(\n",
    "        adata, \n",
    "        color=[\"integration_sample_status\", \"sample_ID\", \"cell_type\", \"concatenated_integration_covariates\"], \n",
    "        ncols=3,\n",
    "        frameon=False,\n",
    "        title=\"\",\n",
    "        legend_fontsize=\"xx-small\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
